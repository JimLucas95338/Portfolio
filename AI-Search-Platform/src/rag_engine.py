"""
Advanced RAG (Retrieval-Augmented Generation) Engine
==================================================

Enterprise-grade RAG implementation with vector search, context understanding,
and intelligent information synthesis for next-generation AI search.

Key Features:
- Vector-based semantic search with multiple embedding models
- Context-aware retrieval with conversation history
- Multi-source information synthesis and ranking
- Source attribution and confidence scoring
- Real-time fact-checking and verification
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json
import asyncio
from datetime import datetime, timedelta
import hashlib
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Simulated imports for production environment
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    import faiss
    import chromadb
except ImportError:
    # For demo purposes - in production these would be actual imports
    print("üì¶ Production dependencies not installed - using simulation mode")
    SentenceTransformer = None
    pipeline = None
    faiss = None
    chromadb = None

@dataclass
class SearchResult:
    """Structured search result with metadata and scoring."""
    content: str
    source: str
    relevance_score: float
    confidence_score: float
    chunk_id: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    timestamp: datetime = None

@dataclass
class RAGResponse:
    """Complete RAG response with synthesis and sources."""
    synthesized_answer: str
    source_results: List[SearchResult]
    confidence_score: float
    query_intent: str
    follow_up_questions: List[str]
    fact_check_status: str
    processing_time_ms: float

class AdvancedRAGEngine:
    """
    Enterprise-grade RAG engine with advanced retrieval and generation capabilities.
    
    Implements state-of-the-art techniques for:
    - Semantic search with multiple embedding models
    - Context-aware retrieval and ranking
    - Multi-source information synthesis
    - Confidence scoring and fact verification
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        
        # Initialize embedding models (multiple for ensemble approach)
        self.embedding_models = self._initialize_embedding_models()
        
        # Initialize vector database
        self.vector_db = self._initialize_vector_db()
        
        # Initialize language model for synthesis
        self.synthesis_model = self._initialize_synthesis_model()
        
        # Conversation context management
        self.conversation_history = defaultdict(list)
        
        # Performance metrics
        self.performance_metrics = {
            'queries_processed': 0,
            'avg_response_time': 0,
            'avg_confidence': 0,
            'cache_hit_rate': 0
        }
        
        # Response cache for performance
        self.response_cache = {}
        
        print(f"üöÄ Advanced RAG Engine initialized")
        print(f"üìä Embedding models: {len(self.embedding_models)}")
        print(f"üóÑÔ∏è Vector database: {type(self.vector_db).__name__}")
        print(f"üß† Synthesis model: {'GPT-4 Compatible' if self.synthesis_model else 'Simulated'}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration for RAG engine."""
        return {
            'embedding_models': ['all-MiniLM-L6-v2', 'all-mpnet-base-v2'],
            'chunk_size': 512,
            'chunk_overlap': 50,
            'max_retrieval_results': 10,
            'synthesis_max_tokens': 500,
            'confidence_threshold': 0.7,
            'cache_ttl_hours': 24,
            'fact_check_enabled': True,
            'conversation_memory_length': 10
        }
    
    def _initialize_embedding_models(self) -> Dict[str, Any]:
        """Initialize multiple embedding models for ensemble retrieval."""
        models = {}
        
        if SentenceTransformer:
            for model_name in self.config['embedding_models']:
                try:
                    models[model_name] = SentenceTransformer(model_name)
                    print(f"‚úÖ Loaded embedding model: {model_name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to load {model_name}: {e}")
        
        # Fallback to simulated models for demo
        if not models:
            models = {
                'simulated_model_1': self._create_simulated_embedding_model(),
                'simulated_model_2': self._create_simulated_embedding_model()
            }
            print("üîß Using simulated embedding models for demonstration")
        
        return models
    
    def _create_simulated_embedding_model(self):
        """Create a simulated embedding model for demo purposes."""
        class SimulatedEmbeddingModel:
            def encode(self, texts, **kwargs):
                # Generate consistent but random-like embeddings
                embeddings = []
                for text in texts:
                    # Use hash for consistency
                    hash_value = int(hashlib.md5(text.encode()).hexdigest(), 16)
                    np.random.seed(hash_value % (2**32))
                    embedding = np.random.normal(0, 1, 384)  # 384-dim embedding
                    embedding = embedding / np.linalg.norm(embedding)  # Normalize
                    embeddings.append(embedding)
                return np.array(embeddings)
        
        return SimulatedEmbeddingModel()
    
    def _initialize_vector_db(self):
        """Initialize vector database for efficient similarity search."""
        if chromadb:
            try:
                client = chromadb.Client()
                collection = client.create_collection("enterprise_search")
                return collection
            except Exception as e:
                print(f"‚ö†Ô∏è ChromaDB initialization failed: {e}")
        
        # Fallback to FAISS or simulated vector DB
        if faiss:
            try:
                # Initialize FAISS index
                dimension = 384  # Standard embedding dimension
                index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
                return index
            except Exception as e:
                print(f"‚ö†Ô∏è FAISS initialization failed: {e}")
        
        # Simulated vector database
        return self._create_simulated_vector_db()
    
    def _create_simulated_vector_db(self):
        """Create simulated vector database for demo."""
        class SimulatedVectorDB:
            def __init__(self):
                self.vectors = []
                self.metadata = []
                self.dimension = 384
            
            def add(self, vectors, metadata_list):
                self.vectors.extend(vectors)
                self.metadata.extend(metadata_list)
            
            def search(self, query_vector, k=10):
                if not self.vectors:
                    return [], []
                
                # Calculate cosine similarity
                similarities = []
                for vector in self.vectors:
                    similarity = np.dot(query_vector, vector) / (
                        np.linalg.norm(query_vector) * np.linalg.norm(vector)
                    )
                    similarities.append(similarity)
                
                # Get top k results
                top_indices = np.argsort(similarities)[-k:][::-1]
                top_scores = [similarities[i] for i in top_indices]
                top_metadata = [self.metadata[i] for i in top_indices]
                
                return top_scores, top_metadata
        
        return SimulatedVectorDB()
    
    def _initialize_synthesis_model(self):
        """Initialize language model for answer synthesis."""
        if pipeline:
            try:
                # Initialize text generation pipeline
                model = pipeline(
                    "text-generation", 
                    model="microsoft/DialoGPT-medium",
                    max_length=500
                )
                return model
            except Exception as e:
                print(f"‚ö†Ô∏è Synthesis model initialization failed: {e}")
        
        # Simulated synthesis for demo
        return self._create_simulated_synthesis_model()
    
    def _create_simulated_synthesis_model(self):
        """Create simulated synthesis model for demo."""
        class SimulatedSynthesisModel:
            def generate_response(self, query, context_chunks):
                # Simulate intelligent synthesis
                context_summary = f"Based on {len(context_chunks)} relevant sources"
                
                # Generate a realistic response template
                if "what" in query.lower():
                    response = f"{context_summary}, here's what I found: [Synthesized information would be generated here based on the retrieved context. The response would combine insights from multiple sources while maintaining accuracy and providing clear attribution.]"
                elif "how" in query.lower():
                    response = f"{context_summary}, here's how to approach this: [Step-by-step guidance would be synthesized from the retrieved content, ensuring practical and actionable information.]"
                elif "why" in query.lower():
                    response = f"{context_summary}, here are the key reasons: [Explanatory content would be generated by analyzing the retrieved information and presenting the underlying causes or rationale.]"
                else:
                    response = f"{context_summary}, here's what you need to know: [Comprehensive answer would be synthesized from the retrieved content, tailored to address the specific query.]"
                
                return response
        
        return SimulatedSynthesisModel()
    
    async def search(self, query: str, user_id: str = "default", filters: Dict = None) -> RAGResponse:
        """
        Perform advanced RAG search with context understanding and synthesis.
        
        Args:
            query: Natural language search query
            user_id: User identifier for conversation context
            filters: Optional filters for search scope
            
        Returns:
            RAGResponse with synthesized answer and source attribution
        """
        start_time = datetime.now()
        
        try:
            # Check cache first
            cache_key = self._generate_cache_key(query, filters)
            if cache_key in self.response_cache:
                cached_response = self.response_cache[cache_key]
                if self._is_cache_valid(cached_response):
                    self.performance_metrics['cache_hit_rate'] += 1
                    return cached_response['response']
            
            # Step 1: Query understanding and intent detection
            query_intent = await self._analyze_query_intent(query)
            
            # Step 2: Context-aware query expansion
            expanded_query = await self._expand_query_with_context(query, user_id)
            
            # Step 3: Multi-model embedding generation
            query_embeddings = await self._generate_query_embeddings(expanded_query)
            
            # Step 4: Semantic search and retrieval
            search_results = await self._perform_semantic_search(
                query_embeddings, filters, self.config['max_retrieval_results']
            )
            
            # Step 5: Result ranking and filtering
            ranked_results = await self._rank_and_filter_results(
                search_results, query, query_intent
            )
            
            # Step 6: Information synthesis
            synthesized_answer = await self._synthesize_response(
                query, ranked_results, query_intent
            )
            
            # Step 7: Confidence scoring and fact-checking
            confidence_score = await self._calculate_confidence_score(
                query, ranked_results, synthesized_answer
            )
            
            fact_check_status = await self._perform_fact_check(
                synthesized_answer, ranked_results
            ) if self.config['fact_check_enabled'] else "disabled"
            
            # Step 8: Generate follow-up questions
            follow_up_questions = await self._generate_follow_up_questions(
                query, synthesized_answer, ranked_results
            )
            
            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Create response
            response = RAGResponse(
                synthesized_answer=synthesized_answer,
                source_results=ranked_results,
                confidence_score=confidence_score,
                query_intent=query_intent,
                follow_up_questions=follow_up_questions,
                fact_check_status=fact_check_status,
                processing_time_ms=processing_time
            )
            
            # Update conversation history
            self._update_conversation_history(user_id, query, response)
            
            # Cache response
            self._cache_response(cache_key, response)
            
            # Update metrics
            self._update_performance_metrics(response)
            
            return response
            
        except Exception as e:
            print(f"‚ùå Error in RAG search: {e}")
            return self._create_error_response(query, str(e))
    
    async def _analyze_query_intent(self, query: str) -> str:
        """Analyze query intent using NLP techniques."""
        query_lower = query.lower()
        
        # Intent classification based on query patterns
        if any(word in query_lower for word in ['what', 'define', 'meaning', 'is']):
            return 'definition'
        elif any(word in query_lower for word in ['how', 'tutorial', 'guide', 'steps']):
            return 'procedural'
        elif any(word in query_lower for word in ['why', 'reason', 'because', 'cause']):
            return 'explanatory'
        elif any(word in query_lower for word in ['where', 'location', 'place']):
            return 'locational'
        elif any(word in query_lower for word in ['when', 'time', 'date', 'schedule']):
            return 'temporal'
        elif any(word in query_lower for word in ['compare', 'difference', 'vs', 'versus']):
            return 'comparative'
        elif any(word in query_lower for word in ['best', 'recommend', 'suggest', 'should']):
            return 'recommendation'
        else:
            return 'informational'
    
    async def _expand_query_with_context(self, query: str, user_id: str) -> str:
        """Expand query using conversation context and domain knowledge."""
        # Get conversation history
        history = self.conversation_history.get(user_id, [])
        
        # Simple context expansion (in production, would use more sophisticated NLP)
        expanded_query = query
        
        if history:
            # Add context from recent conversation
            recent_queries = [item['query'] for item in history[-3:]]
            context_terms = set()
            
            for prev_query in recent_queries:
                # Extract key terms that might provide context
                words = prev_query.lower().split()
                context_terms.update([word for word in words if len(word) > 4])
            
            if context_terms:
                expanded_query = f"{query} (context: {' '.join(list(context_terms)[:5])})"
        
        return expanded_query
    
    async def _generate_query_embeddings(self, query: str) -> Dict[str, np.ndarray]:
        """Generate embeddings using multiple models for ensemble approach."""
        embeddings = {}
        
        for model_name, model in self.embedding_models.items():
            try:
                embedding = model.encode([query])[0]
                embeddings[model_name] = embedding
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating embedding with {model_name}: {e}")
        
        return embeddings
    
    async def _perform_semantic_search(self, query_embeddings: Dict[str, np.ndarray], 
                                     filters: Dict = None, k: int = 10) -> List[SearchResult]:
        """Perform semantic search using ensemble embeddings."""
        all_results = []
        
        for model_name, query_embedding in query_embeddings.items():
            try:
                # Perform search with current embedding
                if hasattr(self.vector_db, 'search'):
                    scores, metadata_list = self.vector_db.search(query_embedding, k=k)
                else:
                    # Simulated search results
                    scores, metadata_list = self._simulate_search_results(query_embedding, k)
                
                # Convert to SearchResult objects
                for i, (score, metadata) in enumerate(zip(scores, metadata_list)):
                    result = SearchResult(
                        content=metadata.get('content', f'Sample content for result {i+1}'),
                        source=metadata.get('source', f'source_{i+1}'),
                        relevance_score=float(score),
                        confidence_score=float(score * 0.9),  # Slightly lower than relevance
                        chunk_id=metadata.get('chunk_id', f'chunk_{model_name}_{i}'),
                        metadata=metadata,
                        embedding=query_embedding,
                        timestamp=datetime.now()
                    )
                    all_results.append(result)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error in semantic search with {model_name}: {e}")
        
        return all_results
    
    def _simulate_search_results(self, query_embedding: np.ndarray, k: int) -> Tuple[List[float], List[Dict]]:
        """Simulate search results for demonstration."""
        # Generate sample results with varying relevance scores
        scores = []
        metadata_list = []
        
        for i in range(k):
            # Simulate decreasing relevance scores
            score = 0.95 - (i * 0.1) + np.random.normal(0, 0.05)
            score = max(0.1, min(1.0, score))  # Keep within bounds
            scores.append(score)
            
            # Create sample metadata
            metadata = {
                'content': f"""This is sample content for search result {i+1}. In a production environment, 
                this would contain the actual retrieved content from your knowledge base, documents, or data sources. 
                The content would be highly relevant to the user's query and provide valuable information 
                for answer synthesis. This content has a relevance score of {score:.2f}.""",
                'source': f'enterprise_doc_{i+1}.pdf',
                'source_type': 'document',
                'chunk_id': f'chunk_{i+1}',
                'section': f'Section {i+1}',
                'last_updated': (datetime.now() - timedelta(days=i*10)).isoformat(),
                'word_count': 150 + i*20,
                'access_level': 'internal' if i % 2 == 0 else 'public'
            }
            metadata_list.append(metadata)
        
        return scores, metadata_list
    
    async def _rank_and_filter_results(self, search_results: List[SearchResult], 
                                     query: str, query_intent: str) -> List[SearchResult]:
        """Rank and filter results based on relevance and quality."""
        # Remove duplicates based on content similarity
        unique_results = self._remove_duplicate_results(search_results)
        
        # Apply intent-based ranking boost
        for result in unique_results:
            intent_boost = self._calculate_intent_boost(result, query_intent)
            result.relevance_score *= intent_boost
        
        # Sort by relevance score
        ranked_results = sorted(unique_results, key=lambda x: x.relevance_score, reverse=True)
        
        # Filter by minimum confidence threshold
        filtered_results = [
            result for result in ranked_results 
            if result.confidence_score >= self.config['confidence_threshold']
        ]
        
        # Return top results
        return filtered_results[:self.config['max_retrieval_results']]
    
    def _remove_duplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Remove duplicate results based on content similarity."""
        unique_results = []
        seen_content_hashes = set()
        
        for result in results:
            # Create content hash for deduplication
            content_hash = hashlib.md5(result.content.encode()).hexdigest()
            
            if content_hash not in seen_content_hashes:
                unique_results.append(result)
                seen_content_hashes.add(content_hash)
        
        return unique_results
    
    def _calculate_intent_boost(self, result: SearchResult, query_intent: str) -> float:
        """Calculate ranking boost based on query intent matching."""
        content_lower = result.content.lower()
        
        intent_keywords = {
            'definition': ['define', 'definition', 'meaning', 'is', 'what'],
            'procedural': ['how', 'step', 'process', 'method', 'procedure'],
            'explanatory': ['why', 'because', 'reason', 'cause', 'explain'],
            'comparative': ['compare', 'difference', 'versus', 'vs', 'better'],
            'recommendation': ['best', 'recommend', 'suggest', 'should', 'advice']
        }
        
        keywords = intent_keywords.get(query_intent, [])
        matches = sum(1 for keyword in keywords if keyword in content_lower)
        
        # Boost factor between 1.0 and 1.3
        boost = 1.0 + (matches * 0.1)
        return min(boost, 1.3)
    
    async def _synthesize_response(self, query: str, results: List[SearchResult], 
                                 query_intent: str) -> str:
        """Synthesize comprehensive response from search results."""
        if not results:
            return "I couldn't find relevant information to answer your query. Please try rephrasing your question or contact support for assistance."
        
        # Prepare context for synthesis
        context_chunks = [result.content for result in results[:5]]  # Top 5 results
        
        # Use synthesis model to generate response
        if hasattr(self.synthesis_model, 'generate_response'):
            synthesized_response = self.synthesis_model.generate_response(query, context_chunks)
        else:
            # Fallback synthesis approach
            synthesized_response = self._fallback_synthesis(query, results, query_intent)
        
        return synthesized_response
    
    def _fallback_synthesis(self, query: str, results: List[SearchResult], 
                          query_intent: str) -> str:
        """Fallback synthesis when advanced model is not available."""
        # Template-based response generation
        top_result = results[0] if results else None
        
        if not top_result:
            return "No relevant information found."
        
        # Intent-based response templates
        if query_intent == 'definition':
            response = f"Based on the available information, here's what I found: {top_result.content[:200]}..."
        elif query_intent == 'procedural':
            response = f"Here's how to approach this: {top_result.content[:200]}..."
        elif query_intent == 'explanatory':
            response = f"The key reasons are: {top_result.content[:200]}..."
        else:
            response = f"Based on {len(results)} relevant sources: {top_result.content[:200]}..."
        
        # Add source attribution
        sources = [result.source for result in results[:3]]
        response += f"\n\nSources: {', '.join(sources)}"
        
        return response
    
    async def _calculate_confidence_score(self, query: str, results: List[SearchResult], 
                                        synthesized_answer: str) -> float:
        """Calculate confidence score for the response."""
        if not results:
            return 0.0
        
        # Factors affecting confidence:
        # 1. Average relevance score of top results
        avg_relevance = np.mean([result.relevance_score for result in results[:3]])
        
        # 2. Number of supporting sources
        source_factor = min(len(results) / 5.0, 1.0)  # Normalize to max 5 sources
        
        # 3. Consistency between sources (simplified)
        consistency_factor = 0.8  # Would be calculated based on content similarity
        
        # 4. Query complexity (simple heuristic)
        query_complexity = min(len(query.split()) / 10.0, 1.0)
        complexity_factor = 1.0 - (query_complexity * 0.2)  # More complex = slightly lower confidence
        
        # Combine factors
        confidence = (avg_relevance * 0.4 + 
                     source_factor * 0.3 + 
                     consistency_factor * 0.2 + 
                     complexity_factor * 0.1)
        
        return round(float(confidence), 3)
    
    async def _perform_fact_check(self, answer: str, sources: List[SearchResult]) -> str:
        """Perform basic fact-checking on the synthesized answer."""
        # In production, this would use advanced fact-checking models
        # For demo, we'll use simple heuristics
        
        if not sources:
            return "insufficient_sources"
        
        # Check if answer is supported by multiple sources
        if len(sources) >= 2:
            # Check for contradictions (simplified)
            high_confidence_sources = [s for s in sources if s.confidence_score > 0.8]
            
            if len(high_confidence_sources) >= 2:
                return "verified"
            else:
                return "partially_verified"
        else:
            return "single_source"
    
    async def _generate_follow_up_questions(self, query: str, answer: str, 
                                          sources: List[SearchResult]) -> List[str]:
        """Generate relevant follow-up questions."""
        # Simple follow-up generation based on query intent and content
        follow_ups = []
        
        query_lower = query.lower()
        
        if 'what' in query_lower:
            follow_ups.append("How does this work in practice?")
            follow_ups.append("What are the key benefits?")
        elif 'how' in query_lower:
            follow_ups.append("What are the prerequisites?")
            follow_ups.append("Are there any alternatives?")
        elif 'why' in query_lower:
            follow_ups.append("What are the implications?")
            follow_ups.append("How can this be addressed?")
        
        # Add source-specific follow-ups
        if sources:
            follow_ups.append("Can you provide more details from the sources?")
            follow_ups.append("What related topics should I explore?")
        
        return follow_ups[:3]  # Limit to 3 follow-ups
    
    def _generate_cache_key(self, query: str, filters: Dict = None) -> str:
        """Generate cache key for query and filters."""
        filter_str = json.dumps(filters or {}, sort_keys=True)
        combined = f"{query}|{filter_str}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    def _is_cache_valid(self, cached_entry: Dict) -> bool:
        """Check if cached entry is still valid."""
        cache_time = cached_entry['timestamp']
        ttl_hours = self.config['cache_ttl_hours']
        
        return datetime.now() - cache_time < timedelta(hours=ttl_hours)
    
    def _cache_response(self, cache_key: str, response: RAGResponse):
        """Cache response for future use."""
        self.response_cache[cache_key] = {
            'response': response,
            'timestamp': datetime.now()
        }
        
        # Limit cache size (simple LRU)
        if len(self.response_cache) > 1000:
            oldest_key = min(self.response_cache.keys(), 
                           key=lambda k: self.response_cache[k]['timestamp'])
            del self.response_cache[oldest_key]
    
    def _update_conversation_history(self, user_id: str, query: str, response: RAGResponse):
        """Update conversation history for context."""
        history_entry = {
            'query': query,
            'intent': response.query_intent,
            'confidence': response.confidence_score,
            'timestamp': datetime.now()
        }
        
        self.conversation_history[user_id].append(history_entry)
        
        # Limit history length
        max_length = self.config['conversation_memory_length']
        if len(self.conversation_history[user_id]) > max_length:
            self.conversation_history[user_id] = self.conversation_history[user_id][-max_length:]
    
    def _update_performance_metrics(self, response: RAGResponse):
        """Update performance metrics."""
        self.performance_metrics['queries_processed'] += 1
        
        # Update average response time
        current_avg = self.performance_metrics['avg_response_time']
        new_time = response.processing_time_ms
        total_queries = self.performance_metrics['queries_processed']
        
        self.performance_metrics['avg_response_time'] = (
            (current_avg * (total_queries - 1) + new_time) / total_queries
        )
        
        # Update average confidence
        current_avg_conf = self.performance_metrics['avg_confidence']
        new_conf = response.confidence_score
        
        self.performance_metrics['avg_confidence'] = (
            (current_avg_conf * (total_queries - 1) + new_conf) / total_queries
        )
    
    def _create_error_response(self, query: str, error_message: str) -> RAGResponse:
        """Create error response for failed queries."""
        return RAGResponse(
            synthesized_answer=f"I encountered an error while processing your query: {error_message}",
            source_results=[],
            confidence_score=0.0,
            query_intent="error",
            follow_up_questions=["Please try rephrasing your question", "Contact support if the issue persists"],
            fact_check_status="error",
            processing_time_ms=0.0
        )
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics."""
        return self.performance_metrics.copy()
    
    def add_documents(self, documents: List[Dict[str, Any]]) -> int:
        """Add documents to the knowledge base."""
        added_count = 0
        
        for doc in documents:
            try:
                # Generate embeddings for document content
                content = doc.get('content', '')
                if not content:
                    continue
                
                embeddings = {}
                for model_name, model in self.embedding_models.items():
                    embedding = model.encode([content])[0]
                    embeddings[model_name] = embedding
                
                # Add to vector database
                if hasattr(self.vector_db, 'add'):
                    # Use first embedding model for storage (could be enhanced)
                    primary_embedding = list(embeddings.values())[0]
                    self.vector_db.add([primary_embedding], [doc])
                
                added_count += 1
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error adding document: {e}")
        
        print(f"‚úÖ Added {added_count} documents to knowledge base")
        return added_count

# Example usage and demonstration
async def main():
    print("üîç Advanced RAG Engine Demo")
    print("=" * 50)
    
    # Initialize RAG engine
    rag_engine = AdvancedRAGEngine()
    
    # Sample documents for knowledge base
    sample_docs = [
        {
            'content': 'Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can analyze data, identify patterns, and make predictions or decisions.',
            'source': 'AI_Fundamentals.pdf',
            'source_type': 'document',
            'section': 'Introduction to ML',
            'last_updated': '2024-01-15'
        },
        {
            'content': 'Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It combines computational linguistics with machine learning to enable machines to process and analyze large amounts of natural language data.',
            'source': 'NLP_Guide.pdf',
            'source_type': 'document',
            'section': 'NLP Overview',
            'last_updated': '2024-01-20'
        },
        {
            'content': 'Vector databases are specialized database systems designed to store and query high-dimensional vector data efficiently. They are essential for applications like semantic search, recommendation systems, and similarity matching in AI applications.',
            'source': 'Vector_Databases.pdf',
            'source_type': 'document',
            'section': 'Database Technology',
            'last_updated': '2024-01-25'
        }
    ]
    
    # Add documents to knowledge base
    rag_engine.add_documents(sample_docs)
    
    # Example queries
    test_queries = [
        "What is machine learning?",
        "How does natural language processing work?",
        "Why are vector databases important for AI?",
        "Compare machine learning and natural language processing"
    ]
    
    print(f"\nüîç Testing RAG Engine with {len(test_queries)} queries:")
    print("-" * 50)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{i}. Query: '{query}'")
        
        # Perform RAG search
        response = await rag_engine.search(query, user_id="demo_user")
        
        print(f"   Intent: {response.query_intent}")
        print(f"   Confidence: {response.confidence_score:.3f}")
        print(f"   Processing time: {response.processing_time_ms:.1f}ms")
        print(f"   Sources: {len(response.source_results)}")
        print(f"   Answer: {response.synthesized_answer[:100]}...")
        
        if response.follow_up_questions:
            print(f"   Follow-ups: {response.follow_up_questions[0]}")
    
    # Display performance metrics
    print(f"\nüìä Performance Metrics:")
    metrics = rag_engine.get_performance_metrics()
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")
    
    print("\n" + "=" * 50)
    print("RAG Engine Demo Complete! üéâ")

if __name__ == "__main__":
    asyncio.run(main())
